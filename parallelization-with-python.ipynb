{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Parallel Processing in Python\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Parallel processing is a mode of operation where tasks are executed simultaneously across multiple processors on the same computer. This approach is designed to reduce overall processing time for computationally intensive operations.\n",
    "\n",
    "However, there is usually a bit of overhead when communicating between processes which can actually increase the overall time taken for small tasks instead of decreasing it. Understanding when and how to apply parallel processing is crucial for effective optimization.\n",
    "\n",
    "In Python, the `multiprocessing` module is used to run independent parallel processes by using subprocesses (instead of threads). It allows you to leverage multiple processors on a machine, which means the processes can be run in completely separate memory locations. This is particularly important in Python due to the Global Interpreter Lock (GIL).\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab you will:\n",
    "\n",
    "* Understand how to structure code and use syntax for parallel processing with `multiprocessing`\n",
    "* Implement both synchronous and asynchronous parallel processing\n",
    "* Learn to parallelize operations on Pandas DataFrames\n",
    "* Solve 3 different use cases using the `multiprocessing.Pool()` interface\n",
    "* Compare performance between serial and parallel implementations\n",
    "\n",
    "## Environment Note\n",
    "\n",
    "This lab is designed to run on your Amazon EC2 instance accessed through VSCode. All multiprocessing features will work correctly in this Linux environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `multiprocessing` (you may need to install it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the maximum number of parallel processes can you run. Note: you don't want to use all the available cores since your computer needs to process other things as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous and Asynchronous execution\n",
    "\n",
    "In parallel processing, there are two types of execution models:\n",
    "\n",
    "* **_Synchronous_** runs the processes in the same order in which they were started. This is achieved by locking the main program until the respective processes are finished.\n",
    "\n",
    "* **_Asynchronous_**, on the other hand, doesn’t involve locking. As a result, the order of results can get mixed up but usually gets done quicker.\n",
    "\n",
    "There are 2 main objects in `multiprocessing` to implement parallel execution of a function: The `Pool` Class and the `Process` Class.\n",
    "\n",
    "### `Pool` Class\n",
    "\n",
    "* Synchronous execution\n",
    "    * `Pool.map()` and `Pool.starmap()`\n",
    "    * `Pool.apply()` not used in lab\n",
    "    \n",
    "* Asynchronous execution\n",
    "    * `Pool.map_async()` and `Pool.starmap_async()`\n",
    "    * `Pool.apply_async())` not used in lab\n",
    "\n",
    "### `Process` Class\n",
    "\n",
    "Let’s take up a typical problem and implement parallelization using the above techniques. In this lab, we stick to the `Pool` class, because it is most convenient to use and serves most common practical applications.\n",
    "\n",
    "More info on these classes [here]((https://docs.python.org/3/library/multiprocessing.html))\n",
    "\n",
    "Read about the differences between apply, map, and starmap [here](https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Given a 2D matrix (or list of lists), count how many numbers are present between a given range in each row. We will transform a matrix into a list of rows. \n",
    "\n",
    "Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.randon.RandomState(100)` to set a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a large numpy array matrix of integers between 0 and 9 with 1,000,000 rows and 5 columns. You can use `np.randon.randint` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(0, 10, size=[1000000, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the array you created in the previous step to a list using `tolist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arr.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore your array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the length of your list is 1,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement solution without parallelization\n",
    "\n",
    "Let’s see how long it takes to compute it without parallelization. For this, we create a function called `howmany_within_range()` and we iterate the function over every row of the matrix. Since we converted the matrix to a list, then we iterate over every element in the list. The function receives all the values on the row (list element) as input and return the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howmany_within_range(row, minimum, maximum):\n",
    "    \"\"\"Returns how many numbers lie within `maximum` and `minimum` in a given `row`\"\"\"\n",
    "    count = 0\n",
    "    for n in row:\n",
    "        if minimum <= n <= maximum:\n",
    "            count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate the function `howmany_within_range` over every row in the matrix and measure the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# TODO: Fill in code here to iterate howmany_within_range over every row\n",
    "# Process each row in data with minimum=4, maximum=8\n",
    "counts = [howmany_within_range(row,4,8) for row in data]\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the first 10 results\n",
    "print(counts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the length of results is 1,000,000 and your implementation didn't cheat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check that the length of results is 1,000,000\n",
    "\n",
    "len(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to parallelize any function?\n",
    "\n",
    "### Platform Note\n",
    "This notebook is designed to run on Amazon EC2 instances accessed through VSCode. The multiprocessing examples will work correctly in this Linux environment.\n",
    "\n",
    "The general way to parallelize any operation is to take a particular function that should be run multiple times and make it run in parallel using different processors.\n",
    "\n",
    "To do this, you initialize a _Pool_ with n number of processors (or cores) and pass the function you want to parallelize to one of _Pool's_ parallelization methods.\n",
    "\n",
    "`multiprocessing.Pool()` provides the `apply()`, `map()` and `starmap()` methods to make any function run in parallel.\n",
    "\n",
    "### Key Differences Between Methods\n",
    "\n",
    "Both `apply` and `map` take the function to be \"parallelized\" as the main argument. But the difference is:\n",
    "- `apply()` takes an _args_ argument that accepts the parameters passed to the _function-to-be-parallelized_ as an argument\n",
    "- `map()` can take only one _iterable_ as an argument\n",
    "\n",
    "This is because `apply()` only runs one worker in the pool (so it's not true parallelization of the function), while `map()` distributes work across all workers in the pool.\n",
    "\n",
    "So `map()` is really more suitable for simpler iterable operations and also does the job faster because it uses all available workers.\n",
    "\n",
    "We will explore `starmap()` once we see how to parallelize `howmany_within_range()` function with `apply()` and `map()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing using `Pool.map()`\n",
    "\n",
    "`Pool.map()` accepts only _one iterable as argument_. So as a workaround, we modify the `howmany_within_range` function by setting a default to the minimum and maximum parameters to create a new `howmany_within_range_rowonly()` function so it accetps only an _iterable list_ of rows as input. This is not a nice use case of map(), but it clearly shows how it differs from apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelizing using Pool.map()\n",
    "\n",
    "# TODO: Redefine function with only 1 mandatory argument\n",
    "# Create howmany_within_range_rowonly() with default parameters\n",
    "\n",
    "start = time.time()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "t1 = time.time()\n",
    "# TODO: Use pool.map function\n",
    "\n",
    "t2 = time.time()\n",
    "pool.close()\n",
    "end = time.time()\n",
    "\n",
    "print('total time',end - start)\n",
    "print('time to set up pool',t1 - start)\n",
    "print('time to multiprocess',t2 - t1)\n",
    "\n",
    "# TODO: Print results after implementing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing using `Pool.starmap()`\n",
    "\n",
    "In previous example, we have to redefine `howmany_within_range` function to make couple of parameters to take default values. Using `starmap()`, you can avoid doing this. How you ask?\n",
    "\n",
    "Like `Pool.map()`, `Pool.starmap()` also accepts only one iterable as argument, but in `starmap()`, each element in that iterable is also a iterable. You can to provide the arguments to the _function-to-be-parallelized_ in the same order in this inner iterable element, will in turn be unpacked during execution. Internally Python is performing `_function-to-be-parallelized_(*args)`, for each iteration, where `args` are the iterable argument supplied to `Pool.starmap`.\n",
    "\n",
    "So effectively, `Pool.starmap()` is like a version of Pool.map() that accepts arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelizing with Pool.starmap()\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# TODO: Use pool.starmap with the original howmany_within_range function\n",
    "counts = pool.starmap(howmany_within_range, ((row, 4, 8) for row in data))\n",
    "pool.close()\n",
    "\n",
    "# TODO: Print results after implementing\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Parallel Processing\n",
    "\n",
    "The asynchronous equivalents `apply_async()`, `map_async()` and `starmap_async()` lets you do execute the processes in parallel asynchronously, that is the next process can start as soon as previous one gets over without regard for the starting order. As a result, there is no guarantee that the result will be in the same order as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing with `Pool.starmap_async()`\n",
    "\n",
    "You saw how `apply_async()` works. Can you imagine and write up an equivalent version for starmap_async and map_async? The implementation is below anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelizing with Pool.starmap_async()\n",
    "\n",
    "def howmany_within_range2(i, row, minimum, maximum):\n",
    "    \"\"\"Returns how many numbers lie within `maximum` and `minimum` in a given `row`\"\"\"\n",
    "    count = 0\n",
    "    for n in row:\n",
    "        if minimum <= n <= maximum:\n",
    "            count = count + 1\n",
    "    return (i, count)\n",
    "\n",
    "import multiprocessing as mp\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# TODO: Use starmap_async to process the data\n",
    "# Remember to use .get() to retrieve results from async call\n",
    "\n",
    "args = ((i, row, 4, 8) for i, row in enumerate(data))\n",
    "async_res = pool.starmap_async(howmany_within_range2, args)\n",
    "\n",
    "results = async_res.get()          \n",
    "pool.close()\n",
    "pool.join()\n",
    "# TODO: Print results after implementing\n",
    "counts = [c for i, c in sorted(results, key=lambda x: x[0])]\n",
    "print(counts[:10])\n",
    "print(len(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Use `Pool.starmap()` to get the row wise common items in list_a and list_b. Each iteration will be row_i in list_a and list_b:\n",
    "\n",
    "```\n",
    "list_a = [[1, 2, 3], [5, 6, 7, 8], [10, 11, 12], [20, 21]]\n",
    "list_b = [[2, 3, 4, 5], [6, 9, 10], [11, 12, 13, 14], [21, 24, 25]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "list_a = [[1, 2, 3], [5, 6, 7, 8], [10, 11, 12], [20, 21]]\n",
    "list_b = [[2, 3, 4, 5], [6, 9, 10], [11, 12, 13, 14], [21, 24, 25]]\n",
    "\n",
    "# TODO: Define a function to find common elements between two lists\n",
    "def common_item(list_a, list_b):\n",
    "    sb = set(list_b)\n",
    "    return [x for x in list_a if x in sb]\n",
    "\n",
    "    pass\n",
    "\n",
    "# TODO: Use Pool.starmap to apply common_item to corresponding rows\n",
    "# Save result as prob1_answer\n",
    " \n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "prob1_answer = pool.starmap(common_item, zip(list_a, list_b))\n",
    "pool.close()\n",
    "pool.join()\n",
    "print(\"Implement the solution to see results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: CPU-Intensive Work Performance Comparison\n",
    "\n",
    "### Overview\n",
    "Compare serial vs parallel execution for CPU-intensive tasks using the `do_busy_work` function from `script.py`.\n",
    "\n",
    "### The do_busy_work Function\n",
    "The `do_busy_work(time_in_seconds: int) -> float` function:\n",
    "- Takes an integer representing seconds of work to simulate\n",
    "- Sleeps for that many seconds (simulating CPU-intensive work)\n",
    "- Prints process ID and timing information\n",
    "- Returns the actual elapsed time as a float\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Your task:\n",
    "\n",
    "1. **Part A - Serial Execution**: \n",
    "   - Process the list `[1, 2, 4, 6]` serially using the built-in `map()` function\n",
    "   - Expected total time: ~13 seconds on an 8-core machine (1+2+4+6)\n",
    "\n",
    "2. **Part B - Parallel with CPU Count**:\n",
    "   - Use `multiprocessing.Pool` with size = `mp.cpu_count()`\n",
    "   - Expected time: ~6 seconds (limited by longest task)\n",
    "\n",
    "3. **Part C - Parallel with Optimal Pool Size**:\n",
    "   - Use `multiprocessing.Pool` with size = `len(work_list)`\n",
    "   - Compare performance with Part B\n",
    "\n",
    "### Key Concepts\n",
    "- Observe how parallel execution reduces total time\n",
    "- Understand relationship between pool size and performance\n",
    "- See why parallel time is limited by the longest individual task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "import multiprocessing as mp\n",
    "from script import do_busy_work\n",
    "\n",
    "work: List = [1, 2, 4, 6]\n",
    "\n",
    "print(\"=== Part A: Serial Execution ===\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "# TODO: Serial execution using map()\n",
    "list(map(do_busy_work,work))\n",
    "\n",
    "time_elapsed = time.perf_counter() - start\n",
    "prob2_part1_answer = time_elapsed\n",
    "print(f\"\\nSerial execution time: {time_elapsed:.2f} seconds\")\n",
    "print(f\"Expected ~13 seconds on an 8-core machine, got {time_elapsed:.2f} seconds\")\n",
    "print(\"\\n=== Part B: Parallel with CPU Count ===\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "# TODO: Parallel execution with CPU count\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    pool.map(do_busy_work, work)\n",
    "\n",
    "time_elapsed = time.perf_counter() - start\n",
    "prob2_part2_answer = time_elapsed\n",
    "print(f\"\\nParallel execution time (CPU count={mp.cpu_count()}): {time_elapsed:.2f} seconds\")\n",
    "\n",
    "print(\"\\n=== Part C: Parallel with Optimal Pool Size ===\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "# TODO: Parallel execution with pool size = work list size\n",
    "with mp.Pool(len(work)) as pool:\n",
    "    pool.map(do_busy_work, work)\n",
    "\n",
    "time_elapsed = time.perf_counter() - start\n",
    "prob2_part3_answer = time_elapsed\n",
    "print(f\"\\nParallel execution time (pool size={len(work)}): {time_elapsed:.2f} seconds\")\n",
    "\n",
    "# TODO: Uncomment after implementing all parts to see performance summary\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"PERFORMANCE SUMMARY\")\n",
    "# print(\"=\"*50)\n",
    "# speedup_cpu = prob2_part1_answer/prob2_part2_answer\n",
    "# speedup_opt = prob2_part1_answer/prob2_part3_answer\n",
    "# print(f\"Serial: {prob2_part1_answer:.2f}s | Parallel (CPU): {prob2_part2_answer:.2f}s ({speedup_cpu:.2f}x) | Parallel (optimal): {prob2_part3_answer:.2f}s ({speedup_opt:.2f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Large-Scale Parallel Data Processing\n",
    "\n",
    "### Overview\n",
    "Real-world data processing often involves operations on large datasets where parallelization can provide significant performance benefits. In this problem, you'll normalize a large dataset and observe how parallel processing scales with data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# TODO: Create your normalization function\n",
    "def normalize(row):\n",
    "    \"\"\"\n",
    "    Normalize a row to [0, 1] range using min-max normalization.\n",
    "    Handle edge case where all values are the same.\n",
    "    \n",
    "    Formula: z_i = (x_i - min(x)) / (max(x) - min(x))\n",
    "    \"\"\"\n",
    "\n",
    "    arr = np.asarray(row,dtype = float)\n",
    "    mini = arr.min()\n",
    "    maxi = arr.max()\n",
    "    ranges = maxi-mini\n",
    "    if ranges == 0:\n",
    "        return [0.0]*len(arr)\n",
    "    \n",
    "    return((arr-mini)/ranges).tolist()\n",
    "\n",
    "    pass\n",
    "\n",
    "# Test with small example first\n",
    "test_data = [\n",
    "    [2, 3, 4, 5],\n",
    "    [6, 9, 10, 12],\n",
    "    [11, 12, 13, 14],\n",
    "    [21, 24, 25, 26],\n",
    "    [100, 100, 100, 100],  # Edge case: all same values\n",
    "]\n",
    "\n",
    "print(\"=== Testing normalization function ===\")\n",
    "# TODO: Test your normalize function with test_data\n",
    "for row in test_data:\n",
    "    print(f\"in:  {row}\")\n",
    "    print(f\"out: {normalize(row)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE COMPARISON WITH DIFFERENT DATASET SIZES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sizes = [\n",
    "    (\"Small\", 100, 50),\n",
    "    (\"Medium\", 1000, 50),\n",
    "    (\"Large\", 10000, 50),\n",
    "    (\"XL\", 50000, 100),\n",
    "    (\"XXL\", 100000, 100),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for size_name, rows, cols in sizes:\n",
    "    print(f\"\\n--- {size_name} Dataset: {rows:,} rows × {cols} columns ---\")\n",
    "    \n",
    "    # Generate random data\n",
    "    data = np.random.randint(0, 1000, size=(rows, cols)).tolist()\n",
    "    \n",
    "    print(f\"Running serial processing...\")\n",
    "    start = time.perf_counter()\n",
    "    # TODO: Serial processing\n",
    "    serial_results = [normalize(row) for row in data]\n",
    "    serial_time = time.perf_counter() - start\n",
    "    print(f\"  Serial time: {serial_time:.4f} seconds\")\n",
    "    \n",
    "    print(f\"Running parallel processing ({mp.cpu_count()} cores)...\")\n",
    "    start = time.perf_counter()\n",
    "    # TODO: Parallel processing\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        parallel_results = pool.map(normalize, data)\n",
    "    parallel_time = time.perf_counter() - start\n",
    "    print(f\"  Parallel time: {parallel_time:.4f} seconds\")\n",
    "    \n",
    "    # TODO: Calculate and display speedup\n",
    "    speedup = (serial_time / parallel_time) if parallel_time > 0 else float(\"inf\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "\n",
    "    # TODO: Store results for summary table\n",
    "    results[size_name] = {\n",
    "        \"rows\": rows,\n",
    "        \"cols\": cols,\n",
    "        \"serial_time\": serial_time,\n",
    "        \"parallel_time\": parallel_time,\n",
    "        \"speedup\": speedup,\n",
    "    }\n",
    "\n",
    "    # Store XXL results for submission\n",
    "    if size_name == \"XXL\":\n",
    "        prob3_answer = parallel_results[:5]\n",
    "\n",
    "# TODO: Display performance summary table and insights after implementation\n",
    "\n",
    "\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Dataset':<8} | {'Rows':>8} | {'Cols':>4} | {'Serial(s)':>10} | {'Parallel(s)':>12} | {'Speedup':>7}\")\n",
    "print(\"-\" * 66)\n",
    "for size_name, rows, cols in sizes:\n",
    "    r = results[size_name]\n",
    "    print(f\"{size_name:<8} | {r['rows']:>8,} | {r['cols']:>4} | {r['serial_time']:>10.4f} | {r['parallel_time']:>12.4f} | {r['speedup']:>7.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save your analytics results to a json object - then add, commit, and push your notebook and json to GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import socket\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# TODO: Make sure all problem variables are defined before running this cell:\n",
    "# - prob1_answer (from Problem 1)\n",
    "# - prob2_part1_answer, prob2_part2_answer, prob2_part3_answer (from Problem 2)\n",
    "# - prob3_answer (from Problem 3)\n",
    "\n",
    "answers: Dict = dict(\n",
    "    prob1=str(prob1_answer),\n",
    "    prob2=dict(part1=prob2_part1_answer, part2=prob2_part2_answer, part3=prob2_part3_answer),\n",
    "    prob3=str(prob3_answer),\n",
    "    host=socket.gethostname()\n",
    ")\n",
    "\n",
    "Path(\"soln.json\").write_text(json.dumps(answers, indent=2))\n",
    "print(\"Results saved to soln.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
